{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"good-start Test whether a project's getting-started documentation is accurate and easy to follow, using an AI agent. Good Start reads your project's setup instructions, attempts to follow them step by step, and reports back with a pass/fail result and actionable feedback. Installation Requires Python 3.12+. pip install good-start Or with uv: uv add good-start Prerequisites Good Start uses the Anthropic API under the hood. Set your API key before running: export ANTHROPIC_API_KEY=your-key-here Quick start Check a project's documentation from the project root: good-start check . Or point it at a specific file: good-start check README.md See the CLI guide for more details, or the pytest plugin to integrate into your test suite.","title":"Home"},{"location":"#good-start","text":"Test whether a project's getting-started documentation is accurate and easy to follow, using an AI agent. Good Start reads your project's setup instructions, attempts to follow them step by step, and reports back with a pass/fail result and actionable feedback.","title":"good-start"},{"location":"#installation","text":"Requires Python 3.12+. pip install good-start Or with uv: uv add good-start","title":"Installation"},{"location":"#prerequisites","text":"Good Start uses the Anthropic API under the hood. Set your API key before running: export ANTHROPIC_API_KEY=your-key-here","title":"Prerequisites"},{"location":"#quick-start","text":"Check a project's documentation from the project root: good-start check . Or point it at a specific file: good-start check README.md See the CLI guide for more details, or the pytest plugin to integrate into your test suite.","title":"Quick start"},{"location":"cli/","text":"CLI The good-start command-line interface is the quickest way to test your project's documentation. Usage Run good-start check from your project root: # Let the agent find the getting-started docs automatically good-start check . # Point it at a specific file good-start check README.md good-start check docs/GETTING_STARTED.md The output is a color-coded pass/fail panel with the agent's findings. Exit codes The CLI returns structured exit codes for use in scripts and CI: 0 \u2014 the agent completed the instructions successfully 1 \u2014 the agent encountered issues following the instructions Saving output Write the results to a file: good-start check . > report.txt Help good-start --help good-start check --help","title":"CLI"},{"location":"cli/#cli","text":"The good-start command-line interface is the quickest way to test your project's documentation.","title":"CLI"},{"location":"cli/#usage","text":"Run good-start check from your project root: # Let the agent find the getting-started docs automatically good-start check . # Point it at a specific file good-start check README.md good-start check docs/GETTING_STARTED.md The output is a color-coded pass/fail panel with the agent's findings.","title":"Usage"},{"location":"cli/#exit-codes","text":"The CLI returns structured exit codes for use in scripts and CI: 0 \u2014 the agent completed the instructions successfully 1 \u2014 the agent encountered issues following the instructions","title":"Exit codes"},{"location":"cli/#saving-output","text":"Write the results to a file: good-start check . > report.txt","title":"Saving output"},{"location":"cli/#help","text":"good-start --help good-start check --help","title":"Help"},{"location":"examples/","text":"Examples A few common ways to use Good Start. Quick check during development Run a one-off check while writing your docs: good-start check README.md Review the output, fix any issues, and run it again. Add to CI with the CLI Add a step to your GitHub Actions workflow: - name: Check getting-started docs env: ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }} run: good-start check . The process exits with code 1 on failure, so your pipeline will catch regressions in your setup instructions. Add to your test suite Use the pytest plugin to make documentation checks part of your regular test runs: # tests/test_docs.py def test_readme(good_start): result = good_start(\"README.md\") assert result.passed, result.details def test_quickstart_guide(good_start): result = good_start(\"docs/QUICKSTART.md\") assert result.passed, result.details Run them: pytest -m good_start -v Or skip them when you want a fast feedback loop: pytest -m \"not good_start\" Check multiple docs with markers Use markers to configure each test independently: import pytest @pytest.mark.good_start(target=\"INSTALL.md\") def test_install_docs(good_start): result = good_start() assert result.passed, result.details @pytest.mark.good_start(target=\"CONTRIBUTING.md\") def test_contributing_docs(good_start): result = good_start() assert result.passed, result.details Custom prompt for stricter checks Write a custom prompt template and pass it in: def test_strict_check(good_start): result = good_start( \"README.md\", prompt_path=\"tests/prompts/strict.md\", ) assert result.passed, result.details This lets you tailor the agent's behavior -- for example, requiring that every command exits cleanly, or that specific tools are installed as part of setup.","title":"Examples"},{"location":"examples/#examples","text":"A few common ways to use Good Start.","title":"Examples"},{"location":"examples/#quick-check-during-development","text":"Run a one-off check while writing your docs: good-start check README.md Review the output, fix any issues, and run it again.","title":"Quick check during development"},{"location":"examples/#add-to-ci-with-the-cli","text":"Add a step to your GitHub Actions workflow: - name: Check getting-started docs env: ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }} run: good-start check . The process exits with code 1 on failure, so your pipeline will catch regressions in your setup instructions.","title":"Add to CI with the CLI"},{"location":"examples/#add-to-your-test-suite","text":"Use the pytest plugin to make documentation checks part of your regular test runs: # tests/test_docs.py def test_readme(good_start): result = good_start(\"README.md\") assert result.passed, result.details def test_quickstart_guide(good_start): result = good_start(\"docs/QUICKSTART.md\") assert result.passed, result.details Run them: pytest -m good_start -v Or skip them when you want a fast feedback loop: pytest -m \"not good_start\"","title":"Add to your test suite"},{"location":"examples/#check-multiple-docs-with-markers","text":"Use markers to configure each test independently: import pytest @pytest.mark.good_start(target=\"INSTALL.md\") def test_install_docs(good_start): result = good_start() assert result.passed, result.details @pytest.mark.good_start(target=\"CONTRIBUTING.md\") def test_contributing_docs(good_start): result = good_start() assert result.passed, result.details","title":"Check multiple docs with markers"},{"location":"examples/#custom-prompt-for-stricter-checks","text":"Write a custom prompt template and pass it in: def test_strict_check(good_start): result = good_start( \"README.md\", prompt_path=\"tests/prompts/strict.md\", ) assert result.passed, result.details This lets you tailor the agent's behavior -- for example, requiring that every command exits cleanly, or that specific tools are installed as part of setup.","title":"Custom prompt for stricter checks"},{"location":"pytest-plugin/","text":"Pytest Plugin Good Start ships as a pytest plugin that is automatically available once installed. No configuration or conftest imports needed. Basic usage def test_getting_started(good_start): result = good_start() assert result.passed, result.details The good_start fixture is a callable that runs the agent and returns a Result object with: result.passed -- boolean indicating whether the agent completed the instructions successfully result.details -- summary of findings, with constructive feedback on failure Checking a specific file def test_quickstart(good_start): result = good_start(\"docs/QUICKSTART.md\") assert result.passed, result.details Configuration Configuration resolves in this order (highest priority first): Direct call arguments: good_start(\"SETUP.md\") Marker keyword arguments: @pytest.mark.good_start(target=\"SETUP.md\") CLI options: pytest --good-start-target=SETUP.md ini options in pyproject.toml Default: \".\" Marker-based configuration import pytest @pytest.mark.good_start(target=\"INSTALL.md\") def test_install_docs(good_start): result = good_start() assert result.passed, result.details Project-wide defaults in pyproject.toml [tool.pytest.ini_options] good_start_target = \"docs/GETTING_STARTED.md\" good_start_prompt = \"tests/prompts/custom.md\" Custom prompt templates Supply your own prompt file to tailor the agent's behavior: def test_with_custom_prompt(good_start): result = good_start(prompt_path=\"tests/prompts/strict.md\") assert result.passed, result.details Skipping agent tests Tests using the good_start fixture are automatically marked with @pytest.mark.good_start . Skip them during fast iteration: pytest -m \"not good_start\" Or run only the documentation tests: pytest -m good_start -v Failure output When a test fails, the agent's detailed findings are appended to the pytest failure report, so you get actionable feedback directly in your CI output.","title":"Pytest Plugin"},{"location":"pytest-plugin/#pytest-plugin","text":"Good Start ships as a pytest plugin that is automatically available once installed. No configuration or conftest imports needed.","title":"Pytest Plugin"},{"location":"pytest-plugin/#basic-usage","text":"def test_getting_started(good_start): result = good_start() assert result.passed, result.details The good_start fixture is a callable that runs the agent and returns a Result object with: result.passed -- boolean indicating whether the agent completed the instructions successfully result.details -- summary of findings, with constructive feedback on failure","title":"Basic usage"},{"location":"pytest-plugin/#checking-a-specific-file","text":"def test_quickstart(good_start): result = good_start(\"docs/QUICKSTART.md\") assert result.passed, result.details","title":"Checking a specific file"},{"location":"pytest-plugin/#configuration","text":"Configuration resolves in this order (highest priority first): Direct call arguments: good_start(\"SETUP.md\") Marker keyword arguments: @pytest.mark.good_start(target=\"SETUP.md\") CLI options: pytest --good-start-target=SETUP.md ini options in pyproject.toml Default: \".\"","title":"Configuration"},{"location":"pytest-plugin/#marker-based-configuration","text":"import pytest @pytest.mark.good_start(target=\"INSTALL.md\") def test_install_docs(good_start): result = good_start() assert result.passed, result.details","title":"Marker-based configuration"},{"location":"pytest-plugin/#project-wide-defaults-in-pyprojecttoml","text":"[tool.pytest.ini_options] good_start_target = \"docs/GETTING_STARTED.md\" good_start_prompt = \"tests/prompts/custom.md\"","title":"Project-wide defaults in pyproject.toml"},{"location":"pytest-plugin/#custom-prompt-templates","text":"Supply your own prompt file to tailor the agent's behavior: def test_with_custom_prompt(good_start): result = good_start(prompt_path=\"tests/prompts/strict.md\") assert result.passed, result.details","title":"Custom prompt templates"},{"location":"pytest-plugin/#skipping-agent-tests","text":"Tests using the good_start fixture are automatically marked with @pytest.mark.good_start . Skip them during fast iteration: pytest -m \"not good_start\" Or run only the documentation tests: pytest -m good_start -v","title":"Skipping agent tests"},{"location":"pytest-plugin/#failure-output","text":"When a test fails, the agent's detailed findings are appended to the pytest failure report, so you get actionable feedback directly in your CI output.","title":"Failure output"}]}